#ifndef HEADER_fd_src_util_sandbox_monitor_seccomp.h_h
#define HEADER_fd_src_util_sandbox_monitor_seccomp.h_h

#include <linux/filter.h>

/* THIS FILE WAS GENERATED BY generate_filters.py. */
/* DO NOT EDIT BY HAND!                            */

static const unsigned int sock_filter_policy_monitor_instr_cnt = 24;

static void populate_sock_filter_policy_monitor(struct sock_filter (*out) [static 24], unsigned int drain_output_fd) {
  *out = {
    /* Check: Jump to RET_KILL_PROCESS if the script's arch != the runtime arch */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, arch ) ) ),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, ARCH_NR, 0, /* RET_KILL_PROCESS */ 20 ),
    /* loading syscall number in accumulator as it might have been evicted by the previous evaluation */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, nr ) ) ),
    /* begin write: (Symbol(or), (Symbol(eq), (Symbol(arg), 0), 1), (Symbol(eq), (Symbol(arg), 0), 2), (Symbol(eq), (Symbol(arg), 0), 3)) */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_write, 0, /* lbl_1 */ 6 ),
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 1, /* RET_ALLOW */ 17, /* lbl_2 */ 0 ),
    /* lbl_2: */
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 2, /* RET_ALLOW */ 15, /* lbl_3 */ 0 ),
    /* lbl_3: */
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 3, /* RET_ALLOW */ 13, /* RET_KILL_PROCESS */ 12 ),
    /* lbl_1: */
    /* loading syscall number in accumulator as it might have been evicted by the previous evaluation */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, nr ) ) ),
    /* begin fsync: (Symbol(eq), (Symbol(arg), 0), 3) */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_fsync, 0, /* lbl_4 */ 2 ),
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 3, /* RET_ALLOW */ 9, /* RET_KILL_PROCESS */ 8 ),
    /* lbl_4: */
    /* loading syscall number in accumulator as it might have been evicted by the previous evaluation */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, nr ) ) ),
    /* simply allow nanosleep */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_nanosleep, /* RET_ALLOW */ 7, 0 ),
    /* simply allow sched_yield */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_sched_yield, /* RET_ALLOW */ 6, 0 ),
    /* simply allow exit_group */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_exit_group, /* RET_ALLOW */ 5, 0 ),
    /* loading syscall number in accumulator as it might have been evicted by the previous evaluation */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, nr ) ) ),
    /* begin read: (Symbol(eq), (Symbol(arg), 0), Symbol(drain_output_fd)) */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_read, 0, /* lbl_5 */ 2 ),
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, drain_output_fd, /* RET_ALLOW */ 1, /* RET_KILL_PROCESS */ 0 ),
    /* lbl_5: */
    /* RET_KILL_PROCESS: */
    /* KILL_PROCESS is placed before ALLOW since it's the fallthrough case. */
    BPF_STMT( BPF_RET | BPF_K, SECCOMP_RET_KILL_PROCESS ),
    /* RET_ALLOW: */
    /* ALLOW has to be reached by jumping */
    BPF_STMT( BPF_RET | BPF_K, SECCOMP_RET_ALLOW ),
  };
}

#endif
