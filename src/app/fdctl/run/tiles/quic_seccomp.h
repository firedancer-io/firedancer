#ifndef HEADER_fd_src_util_sandbox_quic_seccomp.h_h
#define HEADER_fd_src_util_sandbox_quic_seccomp.h_h

#include <linux/filter.h>

/* THIS FILE WAS GENERATED BY generate_filters.py. */
/* DO NOT EDIT BY HAND!                            */

static const unsigned int sock_filter_policy_quic_instr_cnt = 28;

static void populate_sock_filter_policy_quic(struct sock_filter (*out) [static 28]) {
  *out = {
    /* Check: Jump to RET_KILL_PROCESS if the script's arch != the runtime arch */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, arch ) ) ),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, ARCH_NR, 0, /* RET_KILL_PROCESS */ 24 ),
    /* loading syscall number in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, ( offsetof( struct seccomp_data, nr ) ) ),
    /* allow write based on expression */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_write, /* check_write */ 6, 0 ),
    /* allow fsync based on expression */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_fsync, /* check_fsync */ 9, 0 ),
    /* simply allow getpid */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_getpid, /* RET_ALLOW */ 21, 0 ),
    /* simply allow getrandom */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_getrandom, /* RET_ALLOW */ 20, 0 ),
    /* allow madvise based on expression */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_madvise, /* check_madvise */ 8, 0 ),
    /* allow mmap based on expression */
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, __NR_mmap, /* check_mmap */ 9, 0 ),
    /* none of the syscalls matched */
    { BFP_JMP | BPF_JA, 0, 0, /* RET_KILL_PROCESS */ 16 },
//  check_write:
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 2, /* RET_ALLOW */ 15, /* lbl_1 */ 0 ),
//  lbl_1:
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 3, /* RET_ALLOW */ 13, /* RET_KILL_PROCESS */ 12 ),
//  check_fsync:
    /* load syscall argument 0 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[0])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 3, /* RET_ALLOW */ 11, /* RET_KILL_PROCESS */ 10 ),
//  check_madvise:
    /* load syscall argument 2 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[2])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, MADV_DONTNEED, /* RET_ALLOW */ 9, /* RET_KILL_PROCESS */ 8 ),
//  check_mmap:
    /* load syscall argument 2 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[2])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, PROT_WRITE, /* lbl_2 */ 0, /* RET_KILL_PROCESS */ 6 ),
//  lbl_2:
    /* load syscall argument 3 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[3])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, MAP_PRIVATE | MAP_ANONYMOUS, /* lbl_3 */ 0, /* RET_KILL_PROCESS */ 4 ),
//  lbl_3:
    /* load syscall argument 4 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[4])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, -1, /* lbl_4 */ 0, /* RET_KILL_PROCESS */ 2 ),
//  lbl_4:
    /* load syscall argument 5 in accumulator */
    BPF_STMT( BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, args[5])),
    BPF_JUMP( BPF_JMP | BPF_JEQ | BPF_K, 0, /* RET_ALLOW */ 1, /* RET_KILL_PROCESS */ 0 ),
//  RET_KILL_PROCESS:
    /* KILL_PROCESS is placed before ALLOW since it's the fallthrough case. */
    BPF_STMT( BPF_RET | BPF_K, SECCOMP_RET_KILL_PROCESS ),
//  RET_ALLOW:
    /* ALLOW has to be reached by jumping */
    BPF_STMT( BPF_RET | BPF_K, SECCOMP_RET_ALLOW ),
  };
}

#endif
