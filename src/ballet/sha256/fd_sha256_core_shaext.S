# The actual computational loop of this file was autogenerated by the
# sha256 ASM code generator sha256-586.pl and x86asm.pl from OpenSSL
# circa 2022-Dec (open source Apache 2.0 license).  The generated code
# was heavily modified to extract only the SHA extension accelerated
# version needed and then converted to use a modern 64-bit x86_64
# calling convention, exploit extra XMM registers on x86_64, interface
# with the existing library API and expose for library use.  Byte code
# was also replaced with actual Intel ops codes as modern assemblers
# should understand these op codes and it makes the code clearer.

.text
.globl	fd_sha256_core_shaext
.type	fd_sha256_core_shaext,@function
.align	32
fd_sha256_core_shaext:
        # Note: At this point
        # rdi is state
        # rsi is block
        # rdx is block_cnt
        shlq    $6,%rdx
        addq    %rsi,%rdx                                  # rdx = block + block_cnt*64
        leaq    fd_sha256_core_shaext_Kmask+128(%rip),%rax # rax = K + 128, mask at rax + 128 = K + 256 (aligned)
	movdqu	(%rdi),%xmm1
	movdqu	16(%rdi),%xmm2
	movdqa	128(%rax),%xmm7
	pshufd	$27,%xmm1,%xmm0
	pshufd	$177,%xmm1,%xmm1
	pshufd	$27,%xmm2,%xmm2
        palignr	$8,%xmm2,%xmm1
	punpcklqdq	%xmm0,%xmm2
.align	32
.L003loop_shaext:
	movdqu	(%rsi),%xmm3
	movdqu	16(%rsi),%xmm4
	movdqu	32(%rsi),%xmm5
        pshufb  %xmm7,%xmm3
	movdqu	48(%rsi),%xmm6
	movdqa	%xmm2,%xmm9
	movdqa	-128(%rax),%xmm0
	paddd	%xmm3,%xmm0
        pshufb  %xmm7,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	nop
	movdqa	%xmm1,%xmm8
	sha256rnds2	%xmm2,%xmm1
	movdqa	-112(%rax),%xmm0
	paddd	%xmm4,%xmm0
        pshufb  %xmm7,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	leaq	64(%rsi),%rsi
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	-96(%rax),%xmm0
	paddd	%xmm5,%xmm0
        pshufb  %xmm7,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	-80(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	-64(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	-48(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	nop
	paddd	%xmm7,%xmm6
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	-32(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256msg2	%xmm5,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	-16(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	16(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	nop
	paddd	%xmm7,%xmm6
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	32(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256msg2	%xmm5,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	48(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	64(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	80(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	sha256rnds2	%xmm2,%xmm1
	paddd	%xmm7,%xmm6
	movdqa	96(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	sha256msg2	%xmm5,%xmm6
	movdqa	128(%rax),%xmm7
	sha256rnds2	%xmm2,%xmm1
	movdqa	112(%rax),%xmm0
	paddd	%xmm6,%xmm0
	nop
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	cmpq	%rsi,%rdx
	nop
	sha256rnds2	%xmm2,%xmm1
	paddd	%xmm9,%xmm2
	paddd	%xmm8,%xmm1
	jnz	.L003loop_shaext
	pshufd	$177,%xmm2,%xmm2
	pshufd	$27,%xmm1,%xmm7
	pshufd	$177,%xmm1,%xmm1
	punpckhqdq	%xmm2,%xmm1
        palignr	$8,%xmm7,%xmm2
	movdqu	%xmm1,(%rdi)
	movdqu	%xmm2,16(%rdi)
	ret
.size	fd_sha256_core_shaext, .-fd_sha256_core_shaext
        .align 128
        .type   fd_sha256_core_shaext_Kmask, @object
        .size   fd_sha256_core_shaext_Kmask, 272
fd_sha256_core_shaext_Kmask:
.long	1116352408,1899447441,3049323471,3921009573,961987163,1508970993,2453635748,2870763221,3624381080,310598401,607225278,1426881987,1925078388,2162078206,2614888103,3248222580,3835390401,4022224774,264347078,604807628,770255983,1249150122,1555081692,1996064986,2554220882,2821834349,2952996808,3210313671,3336571891,3584528711,113926993,338241895,666307205,773529912,1294757372,1396182291,1695183700,1986661051,2177026350,2456956037,2730485921,2820302411,3259730800,3345764771,3516065817,3600352804,4094571909,275423344,430227734,506948616,659060556,883997877,958139571,1322822218,1537002063,1747873779,1955562222,2024104815,2227730452,2361852424,2428436474,2756734187,3204031479,3329325298,66051,67438087,134810123,202182159
