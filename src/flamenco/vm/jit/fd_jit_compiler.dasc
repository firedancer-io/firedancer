/* WARNING: WORK IN PROGRESS!
   This is an experimental version of the Firedancer JIT compiler.
   It is disabled in production.  There are known security issues in
   this code.  It is not covered by the Firedancer bug bounty policy.

   ### fd_vm_interp compatibility

   fd_jit aims for exact compatibility with fd_vm_interp unless
   otherwise noted.

   ### Optimizations

   The fd_jit transpiler does not implement any optimizations.

   ### Error Handling

   VM faults (e.g. segfault, divide by zero, CU overrun, invalid call
   destination, program EOF, syscall error) are detected reliably and
   deterministically.

   However, after a fault happens, any VM state becomes unreliable
   (non-deterministic or undefined).  Unreliable fault state include CU
   left, current PC, and the error code.

   A VM fault usually triggers ->longjmp, transitioning back into the
   fd_jit_execute call frame. */

#include "../../../util/fd_util_base.h"

/* Include dynasm headers.  These fail to compile when some strict
   checks are enabled. */

void fd_dasm_grow_check( void * ptr, ulong min_sz );
#define DASM_M_GROW(ctx, t, p, sz, need) (fd_dasm_grow_check( (p), (need) ))
#define DASM_M_FREE(ctx, p, sz) do{}while(0)

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wconversion"
#pragma GCC diagnostic ignored "-Wsign-conversion"
#pragma GCC diagnostic ignored "-Wimplicit-fallthrough"
#include "dasm_proto.h"
#include "dasm_x86.h"
#pragma GCC diagnostic pop

#include "fd_jit_private.h"
#include "../fd_vm_private.h"

| .arch x64
| .actionlist actions
| .globals fd_jit_lbl_
| .section code

fd_jit_scratch_layout_t *
fd_jit_scratch_layout( fd_jit_scratch_layout_t * layout,
                       ulong                     bpf_sz ) {

  if( FD_UNLIKELY( bpf_sz > (1UL<<24) ) ) return NULL;
  ulong text_cnt = bpf_sz / 8UL;

  /* These "magic" values are taken from dasm_x86.h */

  ulong dasm_sz     = DASM_PSZ( DASM_MAXSECTION );       /* dasm_x86.h(89) */
  ulong lglabels_sz = (10+fd_jit_lbl__MAX)*sizeof(int);  /* dasm_x86.h(119) */
  ulong pclabels_sz = text_cnt*sizeof(int);              /* dasm_x86.h(127) */
  ulong code_sz     = fd_jit_est_code_sz( bpf_sz );

  memset( layout, 0, sizeof(fd_jit_scratch_layout_t) );
  FD_SCRATCH_ALLOC_INIT( l, 0 );
  layout->dasm_off     = (ulong)FD_SCRATCH_ALLOC_APPEND( l, 16UL, dasm_sz     );
  layout->dasm_sz      = dasm_sz;
  layout->lglabels_off = (ulong)FD_SCRATCH_ALLOC_APPEND( l, 16UL, lglabels_sz );
  layout->lglabels_sz  = lglabels_sz;
  layout->pclabels_off = (ulong)FD_SCRATCH_ALLOC_APPEND( l, 16UL, pclabels_sz );
  layout->pclabels_sz  = pclabels_sz;
  layout->code_off     = (ulong)FD_SCRATCH_ALLOC_APPEND( l, 16UL, code_sz     );
  layout->code_sz      = code_sz;
  layout->sz           = (ulong)FD_SCRATCH_ALLOC_FINI( l, 16UL );
  return layout;
}

dasm_State *
fd_jit_prepare( void *                          scratch,
                fd_jit_scratch_layout_t const * layout ) {

  /* Custom dasm_init */
  dasm_State * d = (void *)( (ulong)scratch + layout->dasm_off );
  fd_memset( d, 0, layout->dasm_sz );
  d->psize      = layout->dasm_sz;
  d->maxsection = DASM_MAXSECTION;

  /* Custom dasm_setupglobal */
  d->globals  = fd_jit_labels;
  d->lglabels = (void *)( (ulong)scratch + layout->lglabels_off );
  d->lgsize   = layout->lglabels_sz;

  /* Custom dasm_growpc */
  d->pcsize   = layout->pclabels_sz;
  d->pclabels = (void *)( (ulong)scratch + layout->pclabels_off );

  /* Setup encoder. Zeros lglabels and pclabels. */
  dasm_setup( &d, actions );

  /* Preallocate space for .code section
     See dasm_x86.h(172) */
  dasm_Section * code = d->sections + 0;
  code->buf   = (void *)( (ulong)scratch + layout->code_off );
  code->bsize = layout->code_sz;
  code->pos   = 0;
  code->rbuf  = code->buf;
  code->epos  = (int)( ((ulong)code->bsize / sizeof(int)) - DASM_MAXSECPOS );
  code->ofs   = 0;

  return d;
}

/* Compile time thread locals */

FD_TL void * fd_jit_labels[ FD_JIT_LABEL_CNT ];
FD_STATIC_ASSERT( sizeof(fd_jit_labels)==fd_jit_lbl__MAX*8, label_cnt );

/* Mapping between sBPF registers and x86_64 registers *****************

   This mapping is valid just before a translated sBPF instruction is
   about to be executed.  (At the `=>next_label` token in the code gen
   loop)

   BPF | r0  | r1  | r2  | r3  | r4  | r5  | r6  | r7  | r8  | r9  | r10
   X86 | rsi | r11 | r12 | r13 | r14 | r15 | rbx | rcx | r8  | r9  | r10

   x86_64 GPRs rax, rdi, rdx, rbp do not map to sBPF registers.  Those can
   be used as scratch registers for complex opcodes.

   Note that this mapping cannot be trivially changed.  Certain x86
   instructions (like div) have hardcoded register accesses which the
   JIT code works around.

   dynasm macros bpf_r{...} resolve to 64-bit register names.

   reg_bpf2x86 is indexed by sBPF register numbers and resolves to the
   x86_64 dynasm register index. */

static uchar const reg_bpf2x86[11] = {
  [ 0] =     FD_DASM_RSI,
  | .define bpf_r0,  rsi
  [ 1] =     FD_DASM_R11,
  | .define bpf_r1,  r11
  [ 2] =     FD_DASM_R12,
  | .define bpf_r2,  r12
  [ 3] =     FD_DASM_R13,
  | .define bpf_r3,  r13
  [ 4] =     FD_DASM_R14,
  | .define bpf_r4,  r14
  [ 5] =     FD_DASM_R15,
  | .define bpf_r5,  r15
  [ 6] =     FD_DASM_RBX,
  | .define bpf_r6,  rbx
  [ 7] =     FD_DASM_RCX,
  | .define bpf_r7,  rcx
  [ 8] =     FD_DASM_R8,
  | .define bpf_r8,  r8
  [ 9] =     FD_DASM_R9,
  | .define bpf_r9,  r9
  [10] =     FD_DASM_R10
  | .define bpf_r10, r10
};


/* JIT compiler *******************************************************/

void
fd_jit_compile( struct dasm_State **       Dst,
                fd_sbpf_program_t const *  prog,
                fd_sbpf_syscalls_t const * syscalls ) {

  int const abiv2 = 0;  /* For now, only support ABIv1 */

  | .code

  /* Derive offsets of thread locals in FS "segment" */

# if defined(__FSGSBASE__)
  ulong fs_base = __builtin_ia32_rdfsbase64();
# else
  ulong fs_base; __asm__( "mov %%fs:0, %0" : "=r"(fs_base) );
# endif
# define FS_RELATIVE(ptr) ((uint)( (ulong)(ptr) - fs_base ))
  uint  fd_jit_vm_tpoff             = FS_RELATIVE( &fd_jit_vm             );
  uint  fd_jit_syscalls_tpoff       = FS_RELATIVE( &fd_jit_syscalls       );
  uint  fd_jit_segment_cnt_tpoff    = FS_RELATIVE( &fd_jit_segment_cnt    );
  uint  fd_jit_mem_sz_tpoff         = FS_RELATIVE( fd_jit_mem_sz          );
  uint  fd_jit_mem_haddr_tpoff      = FS_RELATIVE( fd_jit_mem_haddr       );
  uint  fd_jit_segfault_vaddr_tpoff = FS_RELATIVE( &fd_jit_segfault_vaddr );
  uint  fd_jit_segfault_rip_tpoff   = FS_RELATIVE( &fd_jit_segfault_rip   );
# undef FD_RELATIVE

  |->save_regs:
  | fs
  | mov rax, [fd_jit_vm_tpoff]
  | mov [rax + offsetof(fd_vm_t, reg[ 0])], bpf_r0
  | mov [rax + offsetof(fd_vm_t, reg[ 1])], bpf_r1
  | mov [rax + offsetof(fd_vm_t, reg[ 2])], bpf_r2
  | mov [rax + offsetof(fd_vm_t, reg[ 3])], bpf_r3
  | mov [rax + offsetof(fd_vm_t, reg[ 4])], bpf_r4
  | mov [rax + offsetof(fd_vm_t, reg[ 5])], bpf_r5
  | mov [rax + offsetof(fd_vm_t, reg[ 6])], bpf_r6
  | mov [rax + offsetof(fd_vm_t, reg[ 7])], bpf_r7
  | mov [rax + offsetof(fd_vm_t, reg[ 8])], bpf_r8
  | mov [rax + offsetof(fd_vm_t, reg[ 9])], bpf_r9
  | mov [rax + offsetof(fd_vm_t, reg[10])], bpf_r10
  | ret

  |->restore_regs:
  | fs
  | mov rax, [fd_jit_vm_tpoff]
  | mov bpf_r0,  [rax + offsetof(fd_vm_t, reg[ 0])]
  | mov bpf_r1,  [rax + offsetof(fd_vm_t, reg[ 1])]
  | mov bpf_r2,  [rax + offsetof(fd_vm_t, reg[ 2])]
  | mov bpf_r3,  [rax + offsetof(fd_vm_t, reg[ 3])]
  | mov bpf_r4,  [rax + offsetof(fd_vm_t, reg[ 4])]
  | mov bpf_r5,  [rax + offsetof(fd_vm_t, reg[ 5])]
  | mov bpf_r6,  [rax + offsetof(fd_vm_t, reg[ 6])]
  | mov bpf_r7,  [rax + offsetof(fd_vm_t, reg[ 7])]
  | mov bpf_r8,  [rax + offsetof(fd_vm_t, reg[ 8])]
  | mov bpf_r9,  [rax + offsetof(fd_vm_t, reg[ 9])]
  | mov bpf_r10, [rax + offsetof(fd_vm_t, reg[10])]
  | ret

  /* Helper macros for BPF-to-x86 function calls */

  |.macro enter_x86_frame
  | call ->save_regs
  | mov rbp, rsp
  | and rsp, -16
  |.endmacro

  |.macro leave_x86_frame
  | mov rsp, rbp
  | call ->restore_regs
  |.endmacro

  /* Address translation macros

     The translate_{rw,ro}_{1,2,4,8} macros perform address translation
     and access permission checks for {read-write,read-only} accesses of
     {1,2,4,8} bytes.  The compiler may inline this macro for each
     translated sBPF instruction, so these should be optimized for small
     size.

     Prior to the macro, rdi holds an address in the virtual address
     space (untrusted in [0,2^64)).  If translation and permission
     checks succeed, rdx holds the translated address in the host
     address space.  On failure jumps to sigsegv.  Reasons for failure
     include access to out-of-bounds memory, unaligned address, access
     permission error. */

  | .define translate_in,  rdi
  | .define translate_out, rdx
  /* FIXME make output argument rdi instead of rdx */

  |->fd_jit_vm_translate_ro:
  | mov edx, edi // segment offset
  | shr rdi, 32  // segment index
  | shl edi, 1
  | jmp ->fd_jit_vm_translate

  |->fd_jit_vm_translate_rw:
  | mov edx, edi // segment offset
  | shr rdi, 32  // segment index
  | lea edi, [edi*2+1]
  /* fallthrough */

  |->fd_jit_vm_translate:
  if( abiv2 ) {
    | jmp ->fd_jit_vm_translate_abiv1
  } else {
    | jmp ->fd_jit_vm_translate_abiv2
  }

  /* ABIv1 virtual memory overview

     - There are 6 pages indexed in [0,6)
     - The number of readable bytes in a page is variable in [0,2^31)
     - The number of writable bytes in a page is equal to the number of
       readable bytes or zero
     - The first byte of a page in virtual memory is at (index<<32)
     - Page 0 is always empty (all bytes are unaddressable)
     - Page 2 is 'striped': Virtual addresses with bit 12 set are not
       addressable */

  |->fd_jit_vm_translate_abiv1:
  | // rdi := virtual address
  | // ebp := size of the access minus 1
  |
  | // edx := segment offset
  | mov edx, edi
  |
  | // edi := segment index
  | shr rdi, 32
  |
  | // segment index in bounds?
  | fs
  | cmp edi, [fd_jit_segment_cnt_tpoff]
  | jae ->translate_fail
  |
  | // no multi segment overlap?
  | add ebp, edx
  | jc ->translate_fail
  |
  | // segment offset in bounds?
  | fs
  | cmp edx, [rdi*4 + fd_jit_mem_sz_tpoff]
  | jae ->translate_fail
  |
  | // stack gap?
  | xor ebp, ebp
  | cmp edi, 2
  | mov eax, 0x1000
  | cmove ebp, eax
  | and edx, ebp
  | jae ->translate_fail
  |
  | // rdx := host address
  | fs
  | add rdx, [rdi*8 + fd_jit_mem_haddr_tpoff]
  | ret

  /* ABIv2 virtual memory overview:

     - Virtual memory is described by a page table
     - There can be up to 2^16 pages
     - Each page is 4 GiB (2^32 byte) aligned in virtual memory
     - The number of readable bytes in a page is variable in [0,2^31)
     - The number of writable bytes in a page is less or equal to the
       number of readable bytes */

  |->fd_jit_vm_translate_abiv2:
  | // rdi := virtual address
  | // ebp := size of the access minus 1
  |
  | // segment index in bounds?
  | fs
  | cmp edi, [fd_jit_segment_cnt_tpoff]
  | jae ->translate_fail
  |
  | //// aligned access?
  | // FIXME do we need an alignment check?
  | //mov eax, edx
  | //and eax, ebp
  | //test eax, eax
  | //jnz ->translate_fail
  |
  | // no multi segment overlap?
  | add ebp, edx
  | jc ->translate_fail
  |
  | // segment offset in bounds?
  | fs
  | cmp edx, [rdi*4 + fd_jit_mem_sz_tpoff]
  | jae ->translate_fail
  |
  | // rdx := host address
  | fs
  | add rdx, [rdi*8 + fd_jit_mem_haddr_tpoff]
  | ret

  |->translate_fail:
  | shl rdi, 32
  | or rdi, rdx
  | fs
  | mov [fd_jit_segfault_vaddr_tpoff], rdi
  | mov rdi, [rsp]
  | fs
  | mov [fd_jit_segfault_rip_tpoff], rdi
  | jmp ->vm_fault

  |.macro translate_rw_1
  | xor ebp, ebp
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_2
  | mov ebp, 1
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_4
  | mov ebp, 3
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_8
  | mov ebp, 7
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_ro_1
  | xor ebp, ebp
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_2
  | mov ebp, 1
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_4
  | mov ebp, 3
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_8
  | mov ebp, 7
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  /* Generate setjmp/longjmp subroutines.  These can be called from any
     execution state with a valid stack.  The JIT uses them to restore a
     sane SystemV-ABI context when exiting JIT code.

     These are based on musl libc's setjmp/longjmp implementation.
     Copyright 2011-2012 Nicholas J. Kain, licensed under standard MIT license

     setjmp takes no arguments.  longjmp takes a 64-bit value in rdi.
     When setjmp returns from setjmp, sets rax=0 and rdx=0.  When setjmp
     returns from longjmp, sets rax to the rdi argument of longjmp, and
     sets rdx=1.  setjmp preserves rdi. */

  |->setjmp:
  | mov64 r11, (ulong)fd_jit_jmp_buf
  | mov [r11+ 0], rbx
  | mov [r11+ 8], rbp
  | mov [r11+16], r12
  | mov [r11+24], r13
  | mov [r11+32], r14
  | mov [r11+40], r15
  | // save callee's stack pointer
  | // derived by removing our 8 byte stack frame (only return address)
  | lea rdx, [rsp+8]
  | mov [r11+48], rdx
  | // save return address
  | mov rdx, [rsp]
  | mov [r11+56], rdx
  | // normal return
  | xor eax, eax
  | xor edx, edx
  | ret

  |->longjmp:
  | mov rax, rdi // move first argument to first output register
  | mov edx, 1   // set second output register to 1
  | mov64 rdi, (ulong)fd_jit_jmp_buf
  | // restore execution state to callee of setjmp
  | mov rbx, [rdi+ 0]
  | mov rbp, [rdi+ 8]
  | mov r12, [rdi+16]
  | mov r13, [rdi+24]
  | mov r14, [rdi+32]
  | mov r15, [rdi+40]
  | mov rsp, [rdi+48]
  | push qword [rdi+56]
  | ret // retpoline

  /* The emulate_syscall function switches from a JIT to an interpreter (C)
     execution context and invokes a syscall handler.  Register edi is
     assumed to hold the byte offset into the vm->syscalls table of the
     fd_sbpf_syscalls_t entry to invoke.
     On syscall return, switches back to the JIT execution context and
     resumes execution after the syscall instruction. */

  |->emulate_syscall:
  | call ->save_regs
  | enter_x86_frame
  | sub rsp, 16
  | fs
  | mov r11, [fd_jit_vm_tpoff]
  | fs
  | mov r10, [rdi + fd_jit_syscalls_tpoff + offsetof(fd_sbpf_syscalls_t, func)]
  | mov rdi, r11
  | // load BPF r1 through r5 into function arguments
  | // FIXME could avoid spill to memory by shuffling registers
  | mov rsi, [rax + offsetof(fd_vm_t, reg[1])]
  | mov rdx, [rax + offsetof(fd_vm_t, reg[2])]
  | mov rcx, [rax + offsetof(fd_vm_t, reg[3])]
  | mov r8,  [rax + offsetof(fd_vm_t, reg[4])]
  | mov r9,  [rax + offsetof(fd_vm_t, reg[5])]
  | lea r11, [rax + offsetof(fd_vm_t, reg[0])]
  | push r11
  | call r10
  | test edi, edi
  | jnz ->vm_fault
  | leave_x86_frame
  | ret

  /* The call_stack_push function pushes the current program counter and
     eBPF registers r6, r7, r8, r9 to the shadow stack.  The frame register
     (r10) grows upwards.  FIXME implement shadow stack overflow. */

# define REG(n) (offsetof(fd_vm_t, shadow[0].r##n))

  |->call_stack_push:
  | fs
  | mov rdi, [fd_jit_vm_tpoff]
  | // vm->frame_cnt++
  | inc qword [rdi + offsetof(fd_vm_t, frame_cnt)]
  | // save registers
  | pop rdi
  | push bpf_r6
  | push bpf_r7
  | push bpf_r8
  | push bpf_r9
  | add bpf_r10, 0x2000
  | jmp rdi

  /* The call_stack_pop function undoes the effects of call_stack_push. */

  |->call_stack_pop:
  | fs
  | mov rdi, [fd_jit_vm_tpoff]
  | // vm->frame_cnt--
  | dec qword [rdi + offsetof(fd_vm_t, frame_cnt)]
  | // restore registers
  | pop rdi
  | pop bpf_r9
  | pop bpf_r8
  | pop bpf_r7
  | pop bpf_r6
  | sub bpf_r10, 0x2000
  | jmp rdi

# undef REG
  /* Exception handlers */

  |->vm_fault:
  | mov edi, FD_VM_ERR_SIGABORT
  | jmp ->longjmp

  /* JIT entrypoint from C code */

  |->entrypoint:

  /* Create setjmp anchor used to return from JIT */

  | call ->setjmp // preserves rdi
  | test edx, edx
  | jnz >1

  /* Enter JIT execution context */

  | call ->restore_regs
  | sub rsp, 0x20 // balance call_stack_push
  | call rdi
  | mov rdi, bpf_r0
  | call ->longjmp
  |1:
  | ret

  /* Start translating user code */

  ulong * const text_start = prog->text;
  ulong *       text_end   = prog->text + prog->text_cnt;

  for( ulong * cur=text_start; cur<text_end; cur++ ) {
    ulong instr = *cur;

    ulong opcode  = fd_vm_instr_opcode( instr ); /* in [0,256) even if malformed */
    ulong dst     = fd_vm_instr_dst   ( instr ); /* in [0, 16) even if malformed */
    ulong src     = fd_vm_instr_src   ( instr ); /* in [0, 16) even if malformed */
    ulong offset  = fd_vm_instr_offset( instr ); /* in [-2^15,2^15) even if malformed */
    uint  imm     = fd_vm_instr_imm   ( instr ); /* in [0,2^32) even if malformed */

    /* Macros for translating register accesses */

    uint x86_dst = reg_bpf2x86[ dst ];
    uint x86_src = reg_bpf2x86[ src ];

    | .define dst64, Rq(x86_dst)
    | .define src64, Rq(x86_src)
    | .define dst32, Rd(x86_dst)
    | .define src32, Rd(x86_src)
    | .define src8,  Rb(x86_src)

    /* Macro for translating jumps */

    ulong * jmp_dst       = cur + 1 + offset; /* may be OOB, FIXME validate */
    int     jmp_dst_lbl   = (int)( jmp_dst - text_start );
    //int     jmp_bounds_ok = jmp_dst>=text_start && jmp<text_end;
    /* FIXME do a bounds check */
    /* FIXME what happens if the label is not set? */

    /* FIXME CU accounting */

    /* Create a dynamic label for each instruction */

    uint cur_pc = (uint)( cur - text_start );
    int next_label = (int)cur_pc;
    |=>next_label:

    /* Translate instruction */

    switch( opcode ) {

    /* 0x00 - 0x0f ******************************************************/

    case 0x04:  /* FD_SBPF_OP_ADD_IMM */
      | add dst32, imm
      break;

    case 0x05:  /* FD_SBPF_OP_JA */
      | jmp =>jmp_dst_lbl
      break;

    case 0x07:  /* FD_SBPF_OP_ADD64_IMM */
      | add dst64, imm
      break;

    case 0x0c:  /* FD_SBPF_OP_ADD_REG */
      | add dst32, src32
      break;

    case 0x0f:  /* FD_SBPF_OP_ADD64_REG */
      | add dst64, src64
      break;

    /* 0x10 - 0x1f ******************************************************/

    case 0x14:  /* FD_SBPF_OP_SUB_IMM */
      | sub dst32, imm
      break;

    case 0x15:  /* FD_SBPF_OP_JEQ_IMM */
      | cmp dst64, imm
      /* pre branch check here ... branchless cu update? */
      | je =>jmp_dst_lbl
      break;

    case 0x17:  /* FD_SBPF_OP_SUB64_IMM */
      | sub dst64, imm
      break;

    case 0x18:  /* FD_SBPF_OP_LDQ */
      cur++; {
      ulong imm64 = (ulong)imm | ( (ulong)fd_vm_instr_imm( *cur ) << 32 );
      if( imm64==0 ) {
        | xor dst32, dst32
      } else {
        | mov dst64, imm64
      }
      break;
    }

    case 0x1c:  /* FD_SBPF_OP_SUB_REG */
      | sub dst32, src32
      break;

    case 0x1d:  /* FD_SBPF_OP_JEQ_REG */
      | cmp dst64, src64
      | je =>jmp_dst_lbl
      break;

    case 0x1f:  /* FD_SBPF_OP_SUB64_REG */
      | sub dst64, src64
      break;

    /* 0x20 - 0x2f ******************************************************/

    case 0x24:  /* FD_SBPF_OP_MUL_IMM */
      /* TODO strength reduction? */
      | imul dst32, imm
      break;

    case 0x25:  /* FD_SBPF_OP_JGT_IMM */
      | cmp dst64, imm
      | ja =>jmp_dst_lbl
      break;

    case 0x27:  /* FD_SBPF_OP_MUL64_IMM */
      /* TODO strength reduction? */
      | imul dst64, imm
      break;

    case 0x2c:  /* FD_SBPF_OP_MUL_REG */
      | imul dst32, src32
      break;

    case 0x2d:  /* FD_SBPF_OP_JGT_REG */
      | cmp dst64, src64
      | ja =>jmp_dst_lbl
      break;

    case 0x2f:  /* FD_SBPF_OP_MUL64_REG */
      | imul dst64, src64
      break;

    /* 0x30 - 0x3f ******************************************************/

    case 0x34:  /* FD_SBPF_OP_DIV_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg eax, dst32
      break;

    case 0x35:  /* FD_SBPF_OP_JGE_IMM */
      | cmp dst64, imm
      | jae =>jmp_dst_lbl
      break;

    case 0x37:  /* FD_SBPF_OP_DIV64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x3c:  /* FD_SBPF_OP_DIV_REG */
      | test src32, src32
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg eax, dst32
      break;

    case 0x3d:  /* FD_SBPF_OP_JGE_REG */
      | cmp dst64, src64
      | jae =>jmp_dst_lbl
      break;

    case 0x3f:  /* FD_SBPF_OP_DIV64_REG */
      | test src64, src64
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rax, dst64
      break;

    /* 0x40 - 0x4f ******************************************************/

    case 0x44:  /* FD_SBPF_OP_OR_IMM */
      | or dst32, imm
      break;

    case 0x45:  /* FD_SBPF_OP_JSET_IMM */
      | test dst64, imm
      | jnz =>jmp_dst_lbl
      break;

    case 0x47:  /* FD_SBPF_OP_OR64_IMM */
      | or dst64, imm
      break;

    case 0x4c:  /* FD_SBPF_OP_OR_REG */
      | or dst32, src32
      break;

    case 0x4d:  /* FD_SBPF_OP_JSET_REG */
      | test dst64, src64
      | jnz =>jmp_dst_lbl
      break;

    case 0x4f:  /* FD_SBPF_OP_OR64_REG */
      | or dst64, src64
      break;

    /* 0x50 - 0x5f ******************************************************/

    case 0x54:  /* FD_SBPF_OP_AND_IMM */
      | and dst32, imm
      break;

    case 0x55:  /* FD_SBPF_OP_JNE_IMM */
      | cmp dst64, imm
      | jne =>jmp_dst_lbl
      break;

    case 0x57:  /* FD_SBPF_OP_AND64_IMM */
      | and dst64, imm
      break;

    case 0x5c:  /* FD_SBPF_OP_AND_REG */
      | and dst32, src32
      break;

    case 0x5d:  /* FD_SBPF_OP_JNE_REG */
      | cmp dst64, src64
      | jne =>jmp_dst_lbl
      break;

    case 0x5f:  /* FD_SBPF_OP_AND64_REG */
      | and dst64, src64
      break;

    /* 0x60 - 0x6f ******************************************************/

    case 0x61:  /* FD_SBPF_OP_LDXW */
      | lea translate_in, [src64+offset]
      | translate_ro_4
      | mov dst32, [translate_out]
      break;

    case 0x62:  /* FD_SBPF_OP_STW */
      | lea translate_in, [dst64+offset]
      | translate_rw_4
      | mov dword [translate_out], imm
      break;

    case 0x63:  /* FD_SBPF_OP_STXW */
      | lea translate_in, [dst64+offset]
      | translate_rw_4
      | mov [translate_out], src32
      break;

    case 0x64:  /* FD_SBPF_OP_LSH_IMM */
      | shl dst32, imm
      break;

    case 0x65:  /* FD_SBPF_OP_JSGT_IMM */
      | cmp dst64, imm
      | jg =>jmp_dst_lbl
      break;

    case 0x67:  /* FD_SBPF_OP_LSH64_IMM */
      | shl dst64, imm
      break;

    case 0x69:  /* FD_SBPF_OP_LDXH */
      | lea translate_in, [src64+offset]
      | translate_ro_2
      | xor dst32, dst32
      | mov Rw(x86_dst), [translate_out]
      break;

    case 0x6a:  /* FD_SBPF_OP_STH */
      | lea translate_in, [dst64+offset]
      | translate_rw_2
      | mov word [translate_out], imm
      break;

    case 0x6b:  /* FD_SBPF_OP_STXH */
      | lea translate_in, [dst64+offset]
      | translate_rw_2
      | mov [translate_out], src32
      break;

    case 0x6c:  /* FD_SBPF_OP_LSH_REG */
      | mov cl, src8
      | shl dst32, cl
      break;

    case 0x6d:  /* FD_SBPF_OP_JSGT_REG */
      | cmp dst64, src64
      | jg =>jmp_dst_lbl
      break;

    case 0x6f:  /* FD_SBPF_OP_LSH64_REG */
      | mov cl, src8
      | shl dst64, cl
      break;

    /* 0x70 - 0x7f ******************************************************/

    case 0x71:  /* FD_SBPF_OP_LDXB */
      | lea translate_in, [src64+offset]
      | translate_ro_1
      /* TODO is there a better way to zero upper and mov byte? */
      | xor dst32, dst32
      | mov Rb(x86_dst), [translate_out]
      break;

    case 0x72:  /* FD_SBPF_OP_STB */
      | lea translate_in, [src64+offset]
      | translate_rw_1
      | mov byte [translate_out], imm
      break;

    case 0x73:  /* FD_SBPF_OP_STXB */
      | lea translate_in, [dst64+offset]
      | translate_rw_1
      | mov byte [translate_out], Rb(x86_src)
      break;

    case 0x74:  /* FD_SBPF_OP_RSH_IMM */
      | shr dst32, imm
      break;

    case 0x75:  /* FD_SBPF_OP_JSGE_IMM */
      | cmp dst64, imm
      | jge =>jmp_dst_lbl
      break;

    case 0x77:  /* FD_SBPF_OP_RSH64_IMM */
      | shr dst64, imm
      break;

    case 0x79:  /* FD_SBPF_OP_LDXQ */
      | lea translate_in, [src64+offset]
      | translate_ro_8
      | mov dst64, [translate_out]
      break;

    case 0x7a:  /* FD_SBPF_OP_STQ */
      | lea translate_in, [dst64+offset]
      | translate_rw_8
      | mov dword [translate_out], imm
      | mov dword [translate_out+4], 0
      break;

    case 0x7b:  /* FD_SBPF_OP_STXQ */
      | lea translate_in, [dst64+offset]
      | translate_rw_8
      | mov [translate_out], src64
      break;

    case 0x7c:  /* FD_SBPF_OP_RSH_REG */
      | mov cl, src8
      | shr dst32, cl
      break;

    case 0x7d:  /* FD_SBPF_OP_JSGE_REG */
      | cmp dst64, src64
      | jge =>jmp_dst_lbl
      break;

    case 0x7f:  /* FD_SBPF_OP_RSH64_REG */
      | mov cl, src8
      | shr dst64, cl
      break;

    /* 0x80-0x8f ********************************************************/

    case 0x84:  /* FD_SBPF_OP_NEG */
      | neg dst32
      break;

    case 0x85: { /* FD_SBPF_OP_CALL_IMM */
      fd_sbpf_syscalls_t const * syscall = fd_sbpf_syscalls_query_const( syscalls, imm, NULL );
      if( !syscall ) {
        ulong target_pc = (ulong)fd_pchash_inverse( imm );
        | call ->call_stack_push
        | call =>target_pc
      } else {
        /* Optimize for code footprint: Generate an offset into the
           syscall table (32-bit) instead of the syscall address (64-bit) */
        | mov rdi, (uint)( (ulong)syscall - (ulong)syscalls );
        | call ->emulate_syscall
      }
      break;
    }

    case 0x87:  /* FD_SBPF_OP_NEG64 */
      | neg dst64
      break;

    case 0x8d:  /* FD_SBPF_OP_CALL_REG */
      FD_LOG_WARNING(( "TODO: CALLX" ));
      break;

    /* 0x90 - 0x9f ******************************************************/

    case 0x94:  /* FD_SBPF_OP_MOD_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg edx, dst32
      break;

    case 0x95:  /* FD_SBPF_OP_EXIT */
      | call ->call_stack_pop
      | ret
      break;

    case 0x97:  /* FD_SBPF_OP_MOD64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x9c:  /* FD_SBPF_OP_MOD_REG */
      | test src32, src32
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg edx, dst32
      break;

    case 0x9f:  /* FD_SBPF_OP_MOD64_REG */
      | test src64, src64
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rdx, dst64
      break;

    /* 0xa0 - 0xaf ******************************************************/

    case 0xa4:  /* FD_SBPF_OP_XOR_IMM */
      | xor dst32, imm
      break;

    case 0xa5:  /* FD_SBPF_OP_JLT_IMM */
      | cmp dst64, imm
      | jb =>jmp_dst_lbl
      break;

    case 0xa7:  /* FD_SBPF_OP_XOR64_IMM */
      // TODO sign extension
      | xor dst64, imm
      break;

    case 0xac:  /* FD_SBPF_OP_XOR_REG */
      | xor dst32, src32
      break;

    case 0xad:  /* FD_SBPF_OP_JLT_REG */
      | cmp dst64, src64
      | jb =>jmp_dst_lbl
      break;

    case 0xaf:  /* FD_SBPF_OP_XOR64_REG */
      | xor dst64, src64
      break;

    /* 0xb0 - 0xbf ******************************************************/

    case 0xb4:  /* FD_SBPF_OP_MOV_IMM */
      | mov dst32, imm
      break;

    case 0xb5:  /* FD_SBPF_OP_JLE_IMM */
      | cmp dst64, imm
      | jbe =>jmp_dst_lbl
      break;

    case 0xb7:  /* FD_SBPF_OP_MOV64_IMM */
      if( imm==0 ) {
        | xor dst32, dst32
      } else {
        | mov dst64, imm
      }
      break;

    case 0xbc:  /* FD_SBPF_OP_MOV_REG */
      | mov dst32, src32
      break;

    case 0xbd:  /* FD_SBPF_OP_JLE_REG */
      | cmp dst64, src64
      | jbe =>jmp_dst_lbl
      break;

    case 0xbf:  /* FD_SBPF_OP_MOV64_REG */
      | mov dst64, src64
      break;

    /* 0xc0 - 0xcf ******************************************************/

    case 0xc4:  /* FD_SBPF_OP_ARSH_IMM */
      | sar dst32, imm
      break;

    case 0xc5:  /* FD_SBPF_OP_JSLT_IMM */
      | cmp dst64, imm
      | jl =>jmp_dst_lbl
      break;

    case 0xc7:  /* FD_SBPF_OP_ARSH64_IMM */
      | sar dst64, imm
      break;

    case 0xcc:  /* FD_SBPF_OP_ARSH_REG */
      | mov cl, src8
      | sar dst32, cl
      break;

    case 0xcd:  /* FD_SBPF_OP_JSLT_REG */
      | cmp dst64, src64
      | jl =>jmp_dst_lbl
      break;

    case 0xcf:  /* FD_SBPF_OP_ARSH64_REG */
      | mov cl, src8
      | sar dst64, cl
      break;

    /* 0xd0 - 0xdf ******************************************************/

    case 0xd4:  /* FD_SBPF_OP_END_LE */
      /* nop */
      break;

    case 0xd5:  /* FD_SBPF_OP_JSLE_IMM */
      | cmp dst64, imm
      | jle =>jmp_dst_lbl
      break;

    case 0xdc:  /* FD_SBPF_OP_END_BE */
      switch( imm ) {
      case 16U:
        | movzx dst32, Rw(x86_dst)
        | ror Rw(x86_dst), 8
        break;
      case 32U:
        | bswap dst32
        break;
      case 64U:
        | bswap dst64
        break;
      default:
        break;
        // TODO sigill
      }
      break;

    case 0xdd:  /* FD_SBPF_OP_JSLE_REG */
      | cmp dst64, src64
      | jle =>jmp_dst_lbl
      break;

    default:
      FD_LOG_WARNING(( "Unsupported opcode %lx", opcode ));
      cur = text_end;
      break;

    }

  }

  |->overrun:
  | jmp ->vm_fault

}

fd_jit_entrypoint_t
fd_jit_get_entrypoint( void ) {
  return (fd_jit_entrypoint_t)(ulong)fd_jit_labels[ fd_jit_lbl_entrypoint ];
}
