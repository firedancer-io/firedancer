/* fd_vm_jitproto is a first draft of a sBPF JIT compiler for
   Firedancer.  Nothing to see here, it's broken and work-in-progress. */

#define _GNU_SOURCE
#include "../../fd_flamenco_base.h"

#pragma GCC diagnostic ignored "-Wconversion"
#pragma GCC diagnostic ignored "-Wsign-conversion"

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wimplicit-fallthrough"
#include "dasm_proto.h"
#include "dasm_x86.h"
#pragma GCC diagnostic pop

#include "../../../ballet/sbpf/fd_sbpf_instr.h"
#include "../../../ballet/sbpf/fd_sbpf_loader.h"
#include "../../runtime/fd_acc_mgr.h"
#include "../../runtime/context/fd_exec_epoch_ctx.h"
#include "../../runtime/context/fd_exec_slot_ctx.h"
#include "../../runtime/context/fd_exec_txn_ctx.h"
#include "../../runtime/sysvar/fd_sysvar_recent_hashes.h"
#include "../fd_vm_private.h"

#include <assert.h>
#include <setjmp.h>
#include <errno.h>
#include <stdio.h>
#include <sys/mman.h>
#include <sys/stat.h>

int
main( int     argc,
      char ** argv ) {
  fd_boot( &argc, &argv );
  char const * bin_path = fd_env_strip_cmdline_cstr( &argc, &argv, "--program-file", NULL, NULL );

  /* Read and parse ELF binary */

  FILE * bin_file = fopen( bin_path, "r" );
  if( FD_UNLIKELY( !bin_file ) )
    FD_LOG_ERR(( "fopen(\"%s\") failed (%i-%s)", bin_path, errno, fd_io_strerror( errno ) ));

  struct stat bin_stat;
  if( FD_UNLIKELY( 0!=fstat( fileno( bin_file ), &bin_stat ) ) )
    FD_LOG_ERR(( "fstat() failed (%i-%s)", errno, fd_io_strerror( errno ) ));
  if( FD_UNLIKELY( !S_ISREG( bin_stat.st_mode ) ) )
    FD_LOG_ERR(( "File \"%s\" not a regular file", bin_path ));

  ulong  bin_sz  = (ulong)bin_stat.st_size;
  void * bin_buf = malloc( bin_sz+8UL );
  if( FD_UNLIKELY( !bin_buf ) )
    FD_LOG_ERR(( "malloc(%#lx) failed (%i-%s)", bin_sz, errno, fd_io_strerror( errno ) ));

  if( FD_UNLIKELY( fread( bin_buf, bin_sz, 1UL, bin_file )!=1UL ) )
    FD_LOG_ERR(( "fread() failed (%i-%s)", errno, fd_io_strerror( errno ) ));
  FD_TEST( 0==fclose( bin_file ) );

  int is_deploy = 0;
  fd_sbpf_elf_info_t elf_info;
  FD_TEST( fd_sbpf_elf_peek( &elf_info, bin_buf, bin_sz, is_deploy ) );

  void * rodata = malloc( elf_info.rodata_footprint );
  FD_TEST( rodata );

  ulong  prog_align     = fd_sbpf_program_align();
  ulong  prog_footprint = fd_sbpf_program_footprint( &elf_info );
  fd_sbpf_program_t * prog = fd_sbpf_program_new( aligned_alloc( prog_align, prog_footprint ), &elf_info, rodata );
  FD_TEST( prog );

  fd_sbpf_syscalls_t * syscalls = fd_sbpf_syscalls_new(
      aligned_alloc( fd_sbpf_syscalls_align(), fd_sbpf_syscalls_footprint() ) );
  FD_TEST( syscalls );

  fd_vm_syscall_register_all( syscalls, is_deploy );

  if( FD_UNLIKELY( 0!=fd_sbpf_program_load( prog, bin_buf, bin_sz, syscalls, is_deploy ) ) )
    FD_LOG_ERR(( "fd_sbpf_program_load() failed: %s", fd_sbpf_strerror() ));

  /* Create workspace and scratch allocator */

  ulong cpu_idx = fd_tile_cpu_id( fd_tile_idx() );
  if( cpu_idx>=fd_shmem_cpu_cnt() ) cpu_idx = 0UL;
  fd_wksp_t * wksp = fd_wksp_new_anonymous( FD_SHMEM_NORMAL_PAGE_SZ, 65536, fd_shmem_cpu_idx( fd_shmem_numa_idx( cpu_idx ) ), "wksp", 0UL );
  assert( wksp );

  ulong   smax = 1UL<<30;  /* 1 GiB */
  uchar * smem = malloc( smax );
  ulong fmem[ 64 ];
  fd_scratch_attach( smem, fmem, smax, 64UL );
  fd_scratch_push();

  /* Create runtime context */

  ulong txn_max = 2UL;
  ulong rec_max = 1024UL;
  fd_funk_t * funk = fd_funk_join( fd_funk_new( fd_wksp_alloc_laddr( wksp, fd_funk_align(), fd_funk_footprint(), 1UL ), 1UL, (ulong)fd_tickcount(), txn_max, rec_max ) );
  assert( funk );
  fd_funk_start_write( funk );

  fd_acc_mgr_t * acc_mgr = fd_acc_mgr_new( fd_scratch_alloc( FD_ACC_MGR_ALIGN, FD_ACC_MGR_FOOTPRINT ), funk );
  assert( acc_mgr );

  fd_funk_txn_xid_t xid[1] = {{ .ul = {0,1} }};

  fd_funk_txn_t * funk_txn = fd_funk_txn_prepare( funk, NULL, xid, 1 );
  assert( funk_txn );
  fd_scratch_push();

  ulong vote_acct_max = 1;
  uchar * epoch_ctx_mem = fd_scratch_alloc( fd_exec_epoch_ctx_align(), fd_exec_epoch_ctx_footprint( vote_acct_max ) );
  uchar * slot_ctx_mem  = fd_scratch_alloc( FD_EXEC_SLOT_CTX_ALIGN,  FD_EXEC_SLOT_CTX_FOOTPRINT  );
  uchar * txn_ctx_mem   = fd_scratch_alloc( FD_EXEC_TXN_CTX_ALIGN,   FD_EXEC_TXN_CTX_FOOTPRINT   );

  fd_exec_epoch_ctx_t * epoch_ctx = fd_exec_epoch_ctx_join( fd_exec_epoch_ctx_new( epoch_ctx_mem, vote_acct_max ) );
  fd_exec_slot_ctx_t *  slot_ctx  = fd_exec_slot_ctx_join ( fd_exec_slot_ctx_new ( slot_ctx_mem, fd_scratch_virtual() ) );
  fd_exec_txn_ctx_t *   txn_ctx   = fd_exec_txn_ctx_join  ( fd_exec_txn_ctx_new  ( txn_ctx_mem ) );

  assert( epoch_ctx );
  assert( slot_ctx  );

  epoch_ctx->epoch_bank.rent.lamports_per_uint8_year = 3480;
  epoch_ctx->epoch_bank.rent.exemption_threshold = 2;
  epoch_ctx->epoch_bank.rent.burn_percent = 50;

  fd_features_enable_all( &epoch_ctx->features );

  slot_ctx->epoch_ctx = epoch_ctx;
  slot_ctx->funk_txn  = funk_txn;
  slot_ctx->acc_mgr   = acc_mgr;
  slot_ctx->valloc    = fd_scratch_virtual();

  fd_slot_bank_new( &slot_ctx->slot_bank );
  fd_block_block_hash_entry_t * recent_block_hashes = deq_fd_block_block_hash_entry_t_alloc( slot_ctx->valloc, FD_SYSVAR_RECENT_HASHES_CAP );
  slot_ctx->slot_bank.recent_block_hashes.hashes = recent_block_hashes;
  fd_block_block_hash_entry_t * recent_block_hash = deq_fd_block_block_hash_entry_t_push_tail_nocopy( recent_block_hashes );
  fd_memset( recent_block_hash, 0, sizeof(fd_block_block_hash_entry_t) );

  txn_ctx->epoch_ctx = epoch_ctx;
  txn_ctx->slot_ctx  = slot_ctx;
  txn_ctx->funk_txn  = funk_txn;
  txn_ctx->acc_mgr   = acc_mgr;
  txn_ctx->valloc    = fd_scratch_virtual();

  ulong cu_avail = 10000UL;
  txn_ctx->compute_meter      = cu_avail;
  txn_ctx->compute_unit_limit = cu_avail;

  fd_exec_instr_ctx_t instr_ctx[1] = {{0}};
  instr_ctx->epoch_ctx = epoch_ctx;
  instr_ctx->slot_ctx  = slot_ctx;
  instr_ctx->txn_ctx   = txn_ctx;

  /* Set up VM */

  fd_sha256_t _sha[1];
  fd_sha256_t * sha = fd_sha256_join( fd_sha256_new( _sha ) );

  ulong   input_data_sz = 1000UL;
  uchar * input_data = fd_scratch_alloc( 16UL, input_data_sz );

  fd_vm_input_region_t input_region = {
    .vaddr_offset = 0UL,
    .haddr        = (ulong)input_data,
    .region_sz    = (uint)input_data_sz,
    .is_writable  = 1U
  };
  fd_vm_input_region_t * mem_regions    = &input_region;
  ulong                  mem_region_cnt = 1UL;

  fd_vm_t * vm = fd_vm_join( fd_vm_new( fd_scratch_alloc( fd_vm_align(), fd_vm_footprint() ) ) );
  FD_TEST( vm );
  FD_TEST( fd_vm_init(
    vm,
    instr_ctx,
    FD_VM_HEAP_DEFAULT,  /* heap_max */
    txn_ctx->compute_meter, /* entry_cu */
    rodata, /* rodata */
    elf_info.rodata_sz, /* rodata_sz */
    prog->text, /* text */
    prog->text_cnt, /* text_cnt */
    prog->text_off, /* text_off */
    prog->text_sz,  /* text_sz */
    prog->entry_pc, /* entry_pc */
    prog->calldests, /* calldests */
    syscalls,
    NULL, /* trace */
    sha,
    mem_regions,
    mem_region_cnt,
    0 /* is_deprecated */ ) );

  vm->reg[ 1] = FD_VM_MEM_MAP_INPUT_REGION_START;
  vm->reg[10] = FD_VM_MEM_MAP_STACK_REGION_START + 0x1000;

  printf( "vm at %p\n", (void *)vm );

  /* Set up dynasm */

  dasm_State * d;

  | .arch x64
  | .section code
  dasm_init( &d, DASM_MAXSECTION );

  | .globals lbl_
  void * labels[ lbl__MAX ];
  dasm_setupglobal( &d, labels, lbl__MAX );

  dasm_growpc( &d, prog->text_cnt );
  int next_label = 0;

  | .actionlist actions
  dasm_setup( &d, actions );

  dasm_State ** Dst = &d;

  /* Define the BPF->x86 mapping (static) */

  | .define bpf_r0,  rsi
  | .define bpf_r1,  r11
  | .define bpf_r2,  r12
  | .define bpf_r3,  r13
  | .define bpf_r4,  r14
  | .define bpf_r5,  r15
  | .define bpf_r6,  rbx
  | .define bpf_r7,  rbp
  | .define bpf_r8,  r8
  | .define bpf_r9,  r9
  | .define bpf_r10, r10

  /* Leaves the following registers unoccupied:
     rax, rdi, rcx, rdx */

  /* Define the BPF->x86 mapping (dynamic) */

#define FD_DASM_RAX  (0)
#define FD_DASM_RCX  (1)
#define FD_DASM_RDX  (2)
#define FD_DASM_RBX  (3)
#define FD_DASM_RSP  (4)
#define FD_DASM_RBP  (5)
#define FD_DASM_RSI  (6)
#define FD_DASM_RDI  (7)
#define FD_DASM_R8   (8)
#define FD_DASM_R9   (9)
#define FD_DASM_R10 (10)
#define FD_DASM_R11 (11)
#define FD_DASM_R12 (12)
#define FD_DASM_R13 (13)
#define FD_DASM_R14 (14)
#define FD_DASM_R15 (15)

  uchar reg_bpf2x86[11] = {
    [ 0] = FD_DASM_RSI,
    [ 1] = FD_DASM_R11,
    [ 2] = FD_DASM_R12,
    [ 3] = FD_DASM_R13,
    [ 4] = FD_DASM_R14,
    [ 5] = FD_DASM_R15,
    [ 6] = FD_DASM_RBX,
    [ 7] = FD_DASM_RBP,
    [ 8] = FD_DASM_R8,
    [ 9] = FD_DASM_R9,
    [10] = FD_DASM_R10
  };

  /* Start emitting code */

  | .code

  /* Exception handlers */

  /* TODO */
  |->sigfpe:
  | mov rax, 999
  | jmp ->leave

  /* Start translating user code */

  |->main:

  /* SysV function prologue */

  | push rbp
  | mov rbp, rsp
  | push r15
  | push r14
  | push r13
  | push r12
  | push rbx

  /* Remember the VM pointer */

  | mov64 rbx, (ulong)vm
  | .type aVm, fd_vm_t, rbx

  /* Restore register context */

  | mov bpf_r0,  aVm->reg[ 0]
  | mov bpf_r1,  aVm->reg[ 1]
  | mov bpf_r2,  aVm->reg[ 2]
  | mov bpf_r3,  aVm->reg[ 3]
  | mov bpf_r4,  aVm->reg[ 4]
  | mov bpf_r5,  aVm->reg[ 5]
  | mov bpf_r6,  aVm->reg[ 6]
  | mov bpf_r7,  aVm->reg[ 7]
  | mov bpf_r8,  aVm->reg[ 8]
  | mov bpf_r9,  aVm->reg[ 9]
  | mov bpf_r10, aVm->reg[10]

  ulong * const text_start = prog->text;
  ulong *       text_end   = prog->text + prog->text_cnt;

  int bpf_label_off = next_label;

  for( ulong * cur=text_start; cur<text_end; cur++ ) {
    ulong instr = *cur;

    ulong opcode  = fd_vm_instr_opcode( instr ); /* in [0,256) even if malformed */
    ulong dst     = fd_vm_instr_dst   ( instr ); /* in [0, 16) even if malformed */
    ulong src     = fd_vm_instr_src   ( instr ); /* in [0, 16) even if malformed */
    short offset  = fd_vm_instr_offset( instr ); /* in [-2^15,2^15) even if malformed */
    uint  imm     = fd_vm_instr_imm   ( instr ); /* in [0,2^32) even if malformed */

    /* Macros for translating register accesses */

    uint x86_dst = reg_bpf2x86[ dst ];
    uint x86_src = reg_bpf2x86[ src ];

    | .define dst64, Rq(x86_dst)
    | .define src64, Rq(x86_src)
    | .define dst32, Rd(x86_dst)
    | .define src32, Rd(x86_src)
    | .define src8,  Rb(x86_src)

    /* Macro for translating jumps */

    ulong * jmp_dst       = cur + 1 + offset; /* may be OOB, FIXME validate */
    int     jmp_dst_lbl   = bpf_label_off + (int)( jmp_dst - text_start );
    //int     jmp_bounds_ok = jmp_dst>=text_start && jmp<text_end;
    /* FIXME do a bounds check */
    /* FIXME what happens if the label is not set? */

    /* FIXME CU accounting */

    /* Create a dynamic label for each instruction */

    next_label = bpf_label_off + (int)( text_end - text_start );
    |=>next_label:

    /* Translate instruction */

    switch( opcode ) {

    /* 0x00 - 0x0f ******************************************************/

    case 0x04:  /* FD_SBPF_OP_ADD_IMM */
      | add dst32, imm
      break;

    case 0x05:  /* FD_SBPF_OP_JA */
      | jmp =>jmp_dst_lbl
      break;

    case 0x07:  /* FD_SBPF_OP_ADD64_IMM */
      | add dst64, imm
      break;

    case 0x0c:  /* FD_SBPF_OP_ADD_REG */
      | add dst32, src32
      break;

    case 0x0f:  /* FD_SBPF_OP_ADD64_REG */
      | add dst64, src64
      break;

    /* 0x10 - 0x1f ******************************************************/

    case 0x14:  /* FD_SBPF_OP_SUB_IMM */
      | sub dst32, imm
      break;

    case 0x15:  /* FD_SBPF_OP_JEQ_IMM */
      | cmp dst64, imm
      | je =>jmp_dst_lbl
      break;

    case 0x17:  /* FD_SBPF_OP_SUB64_IMM */
      | sub dst64, imm
      break;

    case 0x18:  /* FD_SBPF_OP_LDQ */
      cur++;
      imm |= ( (ulong)fd_vm_instr_imm( *cur ) << 32 );
      | mov dst64, imm
      break;

    case 0x1c:  /* FD_SBPF_OP_SUB_REG */
      | sub dst32, src32
      break;

    case 0x1d:  /* FD_SBPF_OP_JEQ_REG */
      | cmp dst64, src64
      | je =>jmp_dst_lbl
      break;

    case 0x1f:  /* FD_SBPF_OP_SUB64_REG */
      | sub dst64, src64
      break;

    /* 0x20 - 0x2f ******************************************************/

    case 0x24:  /* FD_SBPF_OP_MUL_IMM */
      /* TODO strength reduction? */
      | imul dst32, imm
      break;

    case 0x25:  /* FD_SBPF_OP_JGT_IMM */
      | cmp dst64, imm
      | ja =>jmp_dst_lbl
      break;

    case 0x27:  /* FD_SBPF_OP_MUL64_IMM */
      /* TODO strength reduction? */
      | imul dst64, imm
      break;

    case 0x2c:  /* FD_SBPF_OP_MUL_REG */
      | imul dst32, src32
      break;

    case 0x2d:  /* FD_SBPF_OP_JGT_REG */
      | cmp dst64, src64
      | ja =>jmp_dst_lbl
      break;

    case 0x2f:  /* FD_SBPF_OP_MUL64_REG */
      | imul dst64, src64
      break;

    /* 0x30 - 0x3f ******************************************************/

    case 0x34:  /* FD_SBPF_OP_DIV_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->sigfpe
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg eax, dst32
      break;

    case 0x35:  /* FD_SBPF_OP_JGE_IMM */
      | cmp dst64, imm
      | jae =>jmp_dst_lbl
      break;

    case 0x37:  /* FD_SBPF_OP_DIV64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->sigfpe
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x3c:  /* FD_SBPF_OP_DIV_REG */
      | test src32, src32
      | jz ->sigfpe
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg eax, dst32
      break;

    case 0x3d:  /* FD_SBPF_OP_JGE_REG */
      | cmp dst64, src64
      | jae =>jmp_dst_lbl
      break;

    case 0x3f:  /* FD_SBPF_OP_DIV64_REG */
      | test src64, src64
      | jz ->sigfpe
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rax, dst64
      break;

    /* 0x40 - 0x4f ******************************************************/

    case 0x44:  /* FD_SBPF_OP_OR_IMM */
      | or dst32, imm
      break;

    case 0x45:  /* FD_SBPF_OP_JSET_IMM */
      | test dst64, imm
      | jnz =>jmp_dst_lbl
      break;

    case 0x47:  /* FD_SBPF_OP_OR64_IMM */
      | or dst64, imm
      break;

    case 0x4c:  /* FD_SBPF_OP_OR_REG */
      | or dst32, src32
      break;

    case 0x4d:  /* FD_SBPF_OP_JSET_REG */
      | test dst64, src64
      | jnz =>jmp_dst_lbl
      break;

    case 0x4f:  /* FD_SBPF_OP_OR64_REG */
      | or dst64, src64
      break;

    /* 0x50 - 0x5f ******************************************************/

    case 0x54:  /* FD_SBPF_OP_AND_IMM */
      | and dst32, imm
      break;

    case 0x55:  /* FD_SBPF_OP_JNE_IMM */
      | cmp dst64, imm
      | jne =>jmp_dst_lbl
      break;

    case 0x57:  /* FD_SBPF_OP_AND64_IMM */
      | and dst64, imm
      break;

    case 0x5c:  /* FD_SBPF_OP_AND_REG */
      | and dst32, src32
      break;

    case 0x5d:  /* FD_SBPF_OP_JNE_REG */
      | cmp dst64, src64
      | jne =>jmp_dst_lbl
      break;

    case 0x5f:  /* FD_SBPF_OP_AND64_REG */
      | and dst64, src64
      break;

    /* 0x60 - 0x6f ******************************************************/

    case 0x61:  /* FD_SBPF_OP_LDXW */
      | lea rdi, [src64+offset]
      // TODO translate
      // TODO check align
      | mov dst32, [rdi]
      break;

    case 0x62:  /* FD_SBPF_OP_STW */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov dword [rdi], imm
      break;

    case 0x63:  /* FD_SBPF_OP_STXW */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov [rdi], src32
      break;

    case 0x64:  /* FD_SBPF_OP_LSH_IMM */
      | shl dst32, imm
      break;

    case 0x65:  /* FD_SBPF_OP_JSGT_IMM */
      | cmp dst64, imm
      | jg =>jmp_dst_lbl
      break;

    case 0x67:  /* FD_SBPF_OP_LSH64_IMM */
      | shl dst64, imm
      break;

    case 0x69:  /* FD_SBPF_OP_LDXH */
      | lea rdi, [src64+offset]
      // TODO translate
      // TODO check align
      break;

    case 0x6a:  /* FD_SBPF_OP_STH */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov word [rdi], imm
      break;

    case 0x6b:  /* FD_SBPF_OP_STXH */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov [rdi], src32
      break;

    case 0x6c:  /* FD_SBPF_OP_LSH_REG */
      | mov cl, src8
      | shl dst32, cl
      break;

    case 0x6d:  /* FD_SBPF_OP_JSGT_REG */
      | cmp dst64, src64
      | jg =>jmp_dst_lbl
      break;

    case 0x6f:  /* FD_SBPF_OP_LSH64_REG */
      | mov cl, src8
      | shl dst64, cl
      break;

    /* 0x70 - 0x7f ******************************************************/

    case 0x71:  /* FD_SBPF_OP_LDXB */
      | lea rdi, [src64+offset]
      // TODO translate
      // TODO check align
      /* TODO is there a better way to zero upper and mov byte? */
      | xor dst32, dst32
      | mov Rb(x86_dst), [rdi]
      break;

    case 0x72:  /* FD_SBPF_OP_STB */
      | lea rdi, [src64+offset]
      // TODO translate
      // TODO check align
      | mov byte [rdi], imm
      break;

    case 0x73:  /* FD_SBPF_OP_STXB */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov byte [rdi], Rb(x86_src)
      break;

    case 0x74:  /* FD_SBPF_OP_RSH_IMM */
      | shr dst32, imm
      break;

    case 0x75:  /* FD_SBPF_OP_JSGE_IMM */
      | cmp dst64, imm
      | jge =>jmp_dst_lbl
      break;

    case 0x77:  /* FD_SBPF_OP_RSH64_IMM */
      | shr dst64, imm
      break;

    case 0x79:  /* FD_SBPF_OP_LDXQ */
      | lea rdi, [src64+offset]
      // TODO translate
      // TODO check align
      | mov dst64, [rdi]
      /* TODO need to skip ahead another instruction */
      break;

    case 0x7a:  /* FD_SBPF_OP_STQ */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov rax, imm
      | mov [rdi], rax
      break;

    case 0x7b:  /* FD_SBPF_OP_STXQ */
      | lea rdi, [dst64+offset]
      // TODO translate
      // TODO check align
      | mov [rdi], src64
      break;

    case 0x7c:  /* FD_SBPF_OP_RSH_REG */
      | mov cl, src8
      | shr dst32, cl
      break;

    case 0x7d:  /* FD_SBPF_OP_JSGE_REG */
      | cmp dst64, src64
      | jge =>jmp_dst_lbl
      break;

    case 0x7f:  /* FD_SBPF_OP_RSH64_REG */
      | mov cl, src8
      | shr dst64, cl
      break;

    /* 0x80-0x8f ********************************************************/

    case 0x84:  /* FD_SBPF_OP_NEG */
      | neg dst32
      break;

    case 0x85:  /* FD_SBPF_OP_CALL_IMM */
      // TODO
      break;

    case 0x87:  /* FD_SBPF_OP_NEG64 */
      | neg dst64
      break;

    case 0x8d:  /* FD_SBPF_OP_CALL_REG */
      // TODO
      break;

    /* 0x90 - 0x9f ******************************************************/

    case 0x94:  /* FD_SBPF_OP_MOD_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->sigfpe
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg edx, dst32
      break;

    case 0x95:  /* FD_SBPF_OP_EXIT */
      | jmp ->leave
      break;

    case 0x97:  /* FD_SBPF_OP_MOD64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->sigfpe
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x9c:  /* FD_SBPF_OP_MOD_REG */
      | test src32, src32
      | jz ->sigfpe
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg edx, dst32
      break;

    case 0x9f:  /* FD_SBPF_OP_MOD64_REG */
      | test src64, src64
      | jz ->sigfpe
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rdx, dst64
      break;

    /* 0xa0 - 0xaf ******************************************************/

    case 0xa4:  /* FD_SBPF_OP_XOR_IMM */
      | xor dst32, imm
      break;

    case 0xa5:  /* FD_SBPF_OP_JLT_IMM */
      | cmp dst64, imm
      | jb =>jmp_dst_lbl
      break;

    case 0xa7:  /* FD_SBPF_OP_XOR64_IMM */
      // TODO sign extension
      | xor dst64, imm
      break;

    case 0xac:  /* FD_SBPF_OP_XOR_REG */
      | xor dst32, src32
      break;

    case 0xad:  /* FD_SBPF_OP_JLT_REG */
      | cmp dst64, src64
      | jb =>jmp_dst_lbl
      break;

    case 0xaf:  /* FD_SBPF_OP_XOR64_REG */
      | xor dst64, src64
      break;

    /* 0xb0 - 0xbf ******************************************************/

    case 0xb4:  /* FD_SBPF_OP_MOV_IMM */
      | mov dst32, imm
      break;

    case 0xb5:  /* FD_SBPF_OP_JLE_IMM */
      | cmp dst64, imm
      | jbe =>jmp_dst_lbl
      break;

    case 0xb7:  /* FD_SBPF_OP_MOV64_IMM */
      | mov dst64, imm
      break;

    case 0xbc:  /* FD_SBPF_OP_MOV_REG */
      | mov dst32, src32
      break;

    case 0xbd:  /* FD_SBPF_OP_JLE_REG */
      | cmp dst64, src64
      | jbe =>jmp_dst_lbl
      break;

    case 0xbf:  /* FD_SBPF_OP_MOV64_REG */
      | mov dst64, src64
      break;

    /* 0xc0 - 0xcf ******************************************************/

    case 0xc4:  /* FD_SBPF_OP_ARSH_IMM */
      | sar dst32, imm
      break;

    case 0xc5:  /* FD_SBPF_OP_JSLT_IMM */
      | cmp dst64, imm
      | jl =>jmp_dst_lbl
      break;

    case 0xc7:  /* FD_SBPF_OP_ARSH64_IMM */
      | sar dst64, imm
      break;

    case 0xcc:  /* FD_SBPF_OP_ARSH_REG */
      | mov cl, src8
      | sar dst32, cl
      break;

    case 0xcd:  /* FD_SBPF_OP_JSLT_REG */
      | cmp dst64, src64
      | jl =>jmp_dst_lbl
      break;

    case 0xcf:  /* FD_SBPF_OP_ARSH64_REG */
      | mov cl, src8
      | sar dst64, cl
      break;

    /* 0xd0 - 0xdf ******************************************************/

    case 0xd4:  /* FD_SBPF_OP_END_LE */
      /* nop */
      break;

    case 0xd5:  /* FD_SBPF_OP_JSLE_IMM */
      | cmp dst64, imm
      | jle =>jmp_dst_lbl
      break;

    case 0xdc:  /* FD_SBPF_OP_END_BE */
      switch( imm ) {
      case 16U:
        | movzx dst32, Rw(x86_dst)
        | ror Rw(x86_dst), 8
        break;
      case 32U:
        | bswap dst32
        break;
      case 64U:
        | bswap dst64
        break;
      default:
        break;
        // TODO sigill
      }
      break;

    case 0xdd:  /* FD_SBPF_OP_JSLE_REG */
      | cmp dst64, src64
      | jle =>jmp_dst_lbl
      break;

    default:
      FD_LOG_WARNING(( "Unsupported opcode %x", opcode ));
      cur = text_end;
      break;

    }

  }

  /* Instruction overrun */

  |->overrun: // FIXME
  | mov rax, 999
  | jmp ->leave

  |->leave:
  | pop rbx
  | pop r12
  | pop r13
  | pop r14
  | pop r15
  | leave
  | ret

  /* Finish genearting code */

  ulong sz;
  dasm_link( &d, &sz );

  void * buf = mmap( 0, sz, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 );
  dasm_encode( &d, buf );
  mprotect( buf, sz, PROT_READ | PROT_EXEC );

  dasm_free( &d );

  /* Execute */

  int (* main_)( void ) = (int (*)( void ))( (ulong)labels[ lbl_main ] );
  printf( "JIT returned %d\n", main_() );

  fd_halt();
  return 0;
}
